{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Data Science Project Docs!","text":"<p>Structuring your data science project according to established standards is paramount to ensure seamless collaboration and facilitate the maintenance and modification of your work by team members.</p> <p>This repository offers a comprehensive example crafted to embrace industry best practices, aimed at enabling the creation of a data science project that is not only robust but also highly reproducible.</p>"},{"location":"#about-repository-structure","title":"About repository structure","text":"<pre><code>\u251c\u2500\u2500\u2500data\n\u2502   \u251c\u2500\u2500\u2500input\n\u2502   \u2502   \u251c\u2500\u2500\u2500datos_a_trabajar.csv\n\u2502   \u2502   \u251c\u2500\u2500\u2500datos_a_trabajar.gzip\n\u2502   \u2502   \u2514\u2500\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500\u2500output\n\u2502       \u251c\u2500\u2500\u2500models_metrics.csv\n\u2502       \u2514\u2500\u2500\u2500uplift.csv\n\u251c\u2500\u2500\u2500ds_project\n\u2502   \u251c\u2500\u2500\u2500notebooks\n\u2502   \u2502   \u251c\u2500\u2500\u2500eda.ipynb\n\u2502   \u2502   \u251c\u2500\u2500\u2500metrics.ipynb\n\u2502   \u2502   \u2514\u2500\u2500\u2500models.ipynb\n\u2502   \u251c\u2500\u2500\u2500metrics.py\n\u2502   \u251c\u2500\u2500\u2500models.py\n\u2502   \u2514\u2500\u2500\u2500utils.py\n\u251c\u2500\u2500\u2500images\n\u2502   \u2514\u2500\u2500\u2500metrics\n\u2502       \u251c\u2500\u2500\u2500curva_roc.png\n\u2502       \u251c\u2500\u2500\u2500feature_importance.png\n\u2502       \u2514\u2500\u2500\u2500uplift.png\n\u251c\u2500\u2500\u2500models\n\u2502   \u2514\u2500\u2500\u2500model.pkl\n\u251c\u2500\u2500\u2500.gitignore\n\u251c\u2500\u2500\u2500poetry.lock\n\u251c\u2500\u2500\u2500poetry.toml\n\u2514\u2500\u2500\u2500\u2500README.md\n</code></pre> <p>Where:</p> <ul> <li>data: Almacena datos utilizados en el proyecto. Suele estar dividido en subcarpetas:</li> <li>input: Contiene datos de entrada, como archivos CSV, bases de datos o cualquier informaci\u00f3n relevante que se utilice para el an\u00e1lisis o modelado.</li> <li> <p>output: Contiene los resultados generados durante el an\u00e1lisis o modelado, como archivos de resultados, m\u00e9tricas, o cualquier otro tipo de salida.</p> </li> <li> <p>ds_project: Directorio principal del proyecto.</p> </li> <li>notebooks: Contiene notebooks de Jupyter, que pueden incluir an\u00e1lisis exploratorio de datos, pruebas de modelos, visualizaciones, entre otros.</li> <li>metrics.py: Un archivo de Python que puede contener funciones o clases para calcular m\u00e9tricas espec\u00edficas utilizadas en el proyecto.</li> <li>models.py: Archivo que puede contener definiciones de modelos de Machine Learning o funciones relacionadas con el entrenamiento y evaluaci\u00f3n de modelos.</li> <li> <p>utils.py: Archivo que alberga funciones o utilidades compartidas en todo el proyecto.</p> </li> <li> <p>images: Almacena im\u00e1genes o gr\u00e1ficos generados durante el an\u00e1lisis de datos, visualizaciones o resultados de modelos.</p> </li> <li> <p>metrics: Carpeta espec\u00edfica para im\u00e1genes relacionadas con las m\u00e9tricas o evaluaci\u00f3n de modelos.</p> </li> <li> <p>models: Contiene modelos entrenados guardados en archivos, como archivos de modelo serializados (por ejemplo, pickle, joblib) o archivos guardados en formatos espec\u00edficos para modelos de Machine Learning.</p> </li> <li>.gitignore: Archivo que enumera los archivos y carpetas que se deben ignorar en el control de versiones mediante Git.</li> <li>poetry.lock: Archivo generado por la herramienta de manejo de dependencias Poetry, que detalla las versiones espec\u00edficas de las dependencias del proyecto.</li> <li>poetry.toml: Archivo de configuraci\u00f3n de Poetry que contiene informaci\u00f3n sobre el proyecto y sus dependencias.</li> <li>README.md: Archivo de documentaci\u00f3n que proporciona informaci\u00f3n general sobre el proyecto, su estructura y c\u00f3mo ejecutarlo.</li> </ul>"},{"location":"#tools-used-in-this-project","title":"Tools used in this project","text":"<p>In this project, we've utilized a variety of powerful tools and libraries to enhance our data science workflow and maintain code quality.</p> <p>Here are some similar examples for the packages you've mentioned, along with relevant articles or documentation:</p> <ol> <li> <p>Poetry (Dependency Management):</p> <ul> <li>Poetry is used to manage project dependencies efficiently.</li> <li>Documentation: Poetry Documentation</li> </ul> </li> <li> <p>Pydantic (Data Validation and Parsing):</p> <ul> <li>Pydantic is employed for data validation and parsing.</li> <li>Documentation: Pydantic Documentation</li> </ul> </li> <li> <p>Mkdocs (Documentation Generation):</p> <ul> <li>Mkdocs is employed to generate project documentation.</li> <li>Documentation: Mkdocs Documentation</li> </ul> </li> <li> <p>Pandas (Data Manipulation):</p> <ul> <li>Pandas is used for data manipulation and analysis.</li> <li>Documentation: Pandas Documentation</li> </ul> </li> <li> <p>Scikit-Learn (Machine Learning):</p> <ul> <li>Scikit-Learn is utilized for machine learning tasks.</li> <li>Documentation: Scikit-Learn Documentation</li> </ul> </li> <li> <p>Pre-commit (Code Review and Formatting):</p> <ul> <li>Pre-commit helps automate code review and formatting tasks.</li> <li>Documentation: Pre-commit Documentation</li> </ul> </li> <li> <p>Black (Code Formatting):</p> <ul> <li>Black is employed for code formatting to ensure consistency.</li> <li>Documentation: Black Documentation</li> </ul> </li> <li> <p>Flake8 (Code Linting):</p> <ul> <li>Flake8 is used for code linting to catch potential issues.</li> <li>Documentation: Flake8 Documentation</li> </ul> </li> <li> <p>Pytest (Testing):</p> <ul> <li>Pytest is used for writing and running tests.</li> <li>Documentation: Pytest Documentation</li> </ul> </li> </ol>"},{"location":"_others_/diagrams2/servicio_web_de_slotting/","title":"Servicio web de slotting","text":"In\u00a0[\u00a0]: Copied! <pre>from diagrams import Diagram, Cluster\nfrom diagrams.programming.language import Python\nfrom diagrams.onprem.inmemory import Redis\nfrom diagrams.onprem.queue import Celery\n</pre> from diagrams import Diagram, Cluster from diagrams.programming.language import Python from diagrams.onprem.inmemory import Redis from diagrams.onprem.queue import Celery In\u00a0[\u00a0]: Copied! <pre>graph_attr = {\"bgcolor\": \"transparent\"}\n</pre> graph_attr = {\"bgcolor\": \"transparent\"} In\u00a0[\u00a0]: Copied! <pre>with Diagram(\"Servicio Web de Slotting\", show=False, graph_attr=graph_attr):\n    with Cluster(\"Slotting Service\"):\n        with Cluster(\"FastAPI Services\"):\n            service = [\n                Python(\"solve\"),\n                Python(\"metrics\"),\n                Python(\"task_status\"),\n                Python(\"...\"),\n            ]\n\n        with Cluster(\"Queue\"):\n            queue = Redis(\"Task Queue\")\n\n        with Cluster(\"Workers\"):\n            workers = [\n                Celery(\"worker_1\"),\n                Celery(\"worker_2\"),\n                Celery(\"...\"),\n                Celery(\"worker_n\"),\n            ]\n\n        service &gt;&gt; queue &gt;&gt; workers\n</pre> with Diagram(\"Servicio Web de Slotting\", show=False, graph_attr=graph_attr):     with Cluster(\"Slotting Service\"):         with Cluster(\"FastAPI Services\"):             service = [                 Python(\"solve\"),                 Python(\"metrics\"),                 Python(\"task_status\"),                 Python(\"...\"),             ]          with Cluster(\"Queue\"):             queue = Redis(\"Task Queue\")          with Cluster(\"Workers\"):             workers = [                 Celery(\"worker_1\"),                 Celery(\"worker_2\"),                 Celery(\"...\"),                 Celery(\"worker_n\"),             ]          service &gt;&gt; queue &gt;&gt; workers"},{"location":"project/description/","title":"Descripci\u00f3n","text":""},{"location":"project/description/#el-desafio","title":"El Desaf\u00edo","text":"<p>El hundimiento del Titanic es uno de los naufragios m\u00e1s infames de la historia.</p> <p>El 15 de abril de 1912, durante su viaje inaugural, el ampliamente considerado \"insubmergible\" RMS Titanic se hundi\u00f3 despu\u00e9s de chocar con un iceberg. Desafortunadamente, no hab\u00eda suficientes botes salvavidas para todos a bordo, lo que result\u00f3 en la muerte de 1502 de los 2224 pasajeros y tripulantes.</p> <p>Si bien hubo cierto elemento de suerte involucrado en sobrevivir, parece que algunos grupos de personas ten\u00edan m\u00e1s probabilidades de sobrevivir que otros.</p> <p>En este desaf\u00edo, te pedimos que construyas un modelo predictivo que responda a la pregunta: \"\u00bfqu\u00e9 tipo de personas ten\u00edan m\u00e1s probabilidades de sobrevivir?\" utilizando datos de pasajeros (es decir, nombre, edad, g\u00e9nero, clase socioecon\u00f3mica, etc.).</p>"},{"location":"project/description/#descripcion-del-dataset","title":"Descripci\u00f3n del Dataset","text":""},{"location":"project/description/#overview","title":"Overview","text":"<p>Los datos se han dividido en dos grupos:</p> <ul> <li>training set (<code>train.csv</code>)</li> <li>test set (<code>test.csv</code>)</li> </ul> <p>The training set debe usarse para construir tus modelos de aprendizaje autom\u00e1tico. Para el conjunto de entrenamiento, proporcionamos el resultado (tambi\u00e9n conocido como \"verdad absoluta\") para cada pasajero. Tu modelo se basar\u00e1 en \"caracter\u00edsticas\" como el g\u00e9nero y la clase de los pasajeros. Tambi\u00e9n puedes usar ingenier\u00eda de caracter\u00edsticas para crear nuevas caracter\u00edsticas.</p> <p>The test set debe usarse para ver qu\u00e9 tan bien funciona tu modelo en datos invisibles. Para el conjunto de prueba, no proporcionamos la verdad absoluta para cada pasajero. Es tu trabajo predecir estos resultados. Para cada pasajero en el conjunto de prueba, usa el modelo que entrenaste para predecir si sobrevivieron o no al hundimiento del Titanic.</p> <p>Tambi\u00e9n incluimos gender_submission.csv, un conjunto de predicciones que asumen que todas y solo las pasajeras mujeres sobreviven, como un ejemplo de c\u00f3mo deber\u00eda ser un archivo de env\u00edo.</p>"},{"location":"project/description/#diccionario-de-datos","title":"Diccionario de Datos","text":"Variable Name Definition Possible Values <code>survival</code> Survival status 0 (No), 1 (Yes) <code>pclass</code> Passenger class 1 (1st), 2 (2nd), 3 (3rd) <code>sex</code> Gender Male, Female <code>age</code> Age in years Numerical <code>sibsp</code> Number of siblings/spouses on board Numerical <code>parch</code> Number of parents/children on board Numerical <code>ticket</code> Ticket number String <code>fare</code> Passenger fare Numerical <code>cabin</code> Cabin number String (may contain missing values) <code>embarked</code> Port of embarkation C (Cherbourg), Q (Queenstown), S (Southampton)"},{"location":"project/description/#variable-notes","title":"Variable Notes","text":"<p>pclass: Un proxy para el estatus socioecon\u00f3mico (SES)</p> <ul> <li>1st = Upper</li> <li>2nd = Middle</li> <li>3rd = Lower</li> </ul> <p>age: La edad es fraccionaria si es menor que 1. Si la edad se estima, est\u00e1 en forma de xx.5</p> <p>sibsp: El conjunto de datos define las relaciones familiares de la siguiente manera...</p> <ul> <li>Sibling = brother, sister, stepbrother, stepsister</li> <li>Spouse = husband, wife (amantes y prometidos fueron ignorados)</li> </ul> <p>parch: El conjunto de datos define las relaciones familiares de la siguiente manera... * Parent =madre, padre * Child = daughter, son, stepdaughter, stepson * Algunos ni\u00f1os viajaron solo con una ni\u00f1era, por lo tanto, parch=0 para ellos.</p> <p>\ud83d\udd11 Nota: Para obtener m\u00e1s detalles sobre el proyecto, consulta la documentaci\u00f3n de Kaggle sobre el desaf\u00edo del Titanic.</p>"},{"location":"project/result_01/","title":"Result 01","text":"In\u00a0[1]: Copied! <pre># librerias\nfrom loguru import logger\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 500)\npd.set_option('display.float_format', lambda x: '%.3f' % x)\n</pre> # librerias from loguru import logger import pandas as pd  import matplotlib.pyplot as plt import seaborn as sns  import warnings warnings.filterwarnings(\"ignore\") pd.set_option('display.max_columns', 500) pd.set_option('display.max_rows', 500) pd.set_option('display.float_format', lambda x: '%.3f' % x) In\u00a0[2]: Copied! <pre>def plot_histogram(data, column, figsize=(8, 4)):\n    \"\"\"\n    Crea y muestra un histograma de la distribuci\u00f3n de los datos de una columna dada en un DataFrame.\n\n    Par\u00e1metros:\n    data (pandas DataFrame): El DataFrame que contiene los datos.\n    column (str): El nombre de la columna para la cual se crear\u00e1 el histograma.\n    figsize (tuple, opcional): El tama\u00f1o de la figura del histograma. Por defecto es (8, 4).\n\n    Returns:\n    None\n    \"\"\"\n\n    # Configurar el estilo de Seaborn\n    sns.set(style='whitegrid')\n\n    # Crear una figura con el tama\u00f1o especificado\n    plt.figure(figsize=figsize)\n\n    # Crear el histograma utilizando Seaborn\n    sns.histplot(data[column].dropna(), bins=20, color='#bce4b5', edgecolor='black')\n\n    # Agregar t\u00edtulo y etiquetas a los ejes\n    plt.title(f'Distribuci\u00f3n de la columna {column}')\n    plt.xlabel(column)\n    plt.ylabel('Frecuencia')\n\n    # Mostrar la cuadr\u00edcula en el gr\u00e1fico\n    plt.grid(True)\n\n    # Mostrar el histograma\n    plt.show()\n</pre> def plot_histogram(data, column, figsize=(8, 4)):     \"\"\"     Crea y muestra un histograma de la distribuci\u00f3n de los datos de una columna dada en un DataFrame.      Par\u00e1metros:     data (pandas DataFrame): El DataFrame que contiene los datos.     column (str): El nombre de la columna para la cual se crear\u00e1 el histograma.     figsize (tuple, opcional): El tama\u00f1o de la figura del histograma. Por defecto es (8, 4).      Returns:     None     \"\"\"      # Configurar el estilo de Seaborn     sns.set(style='whitegrid')      # Crear una figura con el tama\u00f1o especificado     plt.figure(figsize=figsize)      # Crear el histograma utilizando Seaborn     sns.histplot(data[column].dropna(), bins=20, color='#bce4b5', edgecolor='black')      # Agregar t\u00edtulo y etiquetas a los ejes     plt.title(f'Distribuci\u00f3n de la columna {column}')     plt.xlabel(column)     plt.ylabel('Frecuencia')      # Mostrar la cuadr\u00edcula en el gr\u00e1fico     plt.grid(True)      # Mostrar el histograma     plt.show() In\u00a0[3]: Copied! <pre>def plot_histogram_vo(data, column, vo, figsize=(8, 4)):\n    \"\"\"\n    Crea y muestra un histograma de la distribuci\u00f3n de los datos de una columna dada en un DataFrame,\n    dividido por una variable categ\u00f3rica (variable objetivo).\n\n    Par\u00e1metros:\n    data (pandas DataFrame): El DataFrame que contiene los datos.\n    column (str): El nombre de la columna para la cual se crear\u00e1 el histograma.\n    vo (str): El nombre de la variable objetivo para dividir los datos en el histograma.\n    figsize (tuple, opcional): El tama\u00f1o de la figura del histograma. Por defecto es (8, 4).\n\n    Returns:\n    None\n    \"\"\"\n\n    # Configurar el estilo de Seaborn\n    sns.set(style='whitegrid')\n\n    # Crear una figura con el tama\u00f1o especificado\n    plt.figure(figsize=figsize)\n\n    # Crear histogramas utilizando Seaborn\n    sns.histplot(x=column, hue=vo, data=data, palette='Greens', edgecolor='black')\n\n    # Agregar t\u00edtulo y etiquetas a los ejes\n    plt.title(f'Distribuci\u00f3n de la columna {column}')\n    plt.xlabel(column)\n    plt.ylabel('Frecuencia')\n\n    # Mostrar el histograma\n    plt.show()\n</pre> def plot_histogram_vo(data, column, vo, figsize=(8, 4)):     \"\"\"     Crea y muestra un histograma de la distribuci\u00f3n de los datos de una columna dada en un DataFrame,     dividido por una variable categ\u00f3rica (variable objetivo).      Par\u00e1metros:     data (pandas DataFrame): El DataFrame que contiene los datos.     column (str): El nombre de la columna para la cual se crear\u00e1 el histograma.     vo (str): El nombre de la variable objetivo para dividir los datos en el histograma.     figsize (tuple, opcional): El tama\u00f1o de la figura del histograma. Por defecto es (8, 4).      Returns:     None     \"\"\"      # Configurar el estilo de Seaborn     sns.set(style='whitegrid')      # Crear una figura con el tama\u00f1o especificado     plt.figure(figsize=figsize)      # Crear histogramas utilizando Seaborn     sns.histplot(x=column, hue=vo, data=data, palette='Greens', edgecolor='black')      # Agregar t\u00edtulo y etiquetas a los ejes     plt.title(f'Distribuci\u00f3n de la columna {column}')     plt.xlabel(column)     plt.ylabel('Frecuencia')      # Mostrar el histograma     plt.show() In\u00a0[4]: Copied! <pre>def plot_range_distribution(data, column, bins, figsize=(8, 4)):\n    \"\"\"\n    Crea y muestra un gr\u00e1fico de barras que representa la distribuci\u00f3n de una columna\n    dividida en rangos espec\u00edficos en un DataFrame.\n\n    Par\u00e1metros:\n    data (pandas DataFrame): El DataFrame que contiene los datos.\n    column (str): El nombre de la columna para la cual se crear\u00e1 la distribuci\u00f3n por rangos.\n    bins (int or sequence of scalars): El n\u00famero de contenedores (bins) o los l\u00edmites de los contenedores para la divisi\u00f3n.\n    figsize (tuple, opcional): El tama\u00f1o de la figura del gr\u00e1fico de barras. Por defecto es (8, 4).\n\n    Returns:\n    None\n    \"\"\"\n\n    # Configurar el estilo de Seaborn\n    sns.set(style='whitegrid')\n\n    # Agregar una nueva columna al DataFrame con los rangos de la columna espec\u00edfica\n    data[column + 'Range'] = pd.cut(data[column], bins=bins, right=False)\n\n    # Contar el n\u00famero de elementos en cada rango\n    temp_counts = data[column + 'Range'].value_counts().sort_index()\n\n    # Calcular los porcentajes en lugar de contar el n\u00famero de elementos\n    temp_percentages = (temp_counts / temp_counts.sum()) * 100  # Calcular los porcentajes relativos\n\n    # Crear el gr\u00e1fico de barras\n    plt.figure(figsize=figsize)\n    sns.barplot(x=temp_percentages.index, y=temp_percentages.values, color='#bce4b5', edgecolor='black')\n\n    # A\u00f1adir anotaciones de valores en las barras (porcentajes)\n    for i, value in enumerate(temp_percentages):\n        plt.text(i, value + 0.2, f'{value:.2f}%', ha='center', va='bottom', fontsize=9)\n\n    # Establecer t\u00edtulo y etiquetas de los ejes\n    plt.title(f'Distribuci\u00f3n por Rango de la columna {column}')\n    plt.xlabel('Rangos')\n    plt.ylabel('Frecuencia')\n    plt.xticks(rotation=0)  # Rotar las etiquetas del eje x para mayor legibilidad\n    plt.tight_layout()\n\n    # Mostrar el gr\u00e1fico\n    plt.show()\n\n    # eliminar columna extra\n    data.drop(column + 'Range',axis=1,inplace=True)\n</pre> def plot_range_distribution(data, column, bins, figsize=(8, 4)):     \"\"\"     Crea y muestra un gr\u00e1fico de barras que representa la distribuci\u00f3n de una columna     dividida en rangos espec\u00edficos en un DataFrame.      Par\u00e1metros:     data (pandas DataFrame): El DataFrame que contiene los datos.     column (str): El nombre de la columna para la cual se crear\u00e1 la distribuci\u00f3n por rangos.     bins (int or sequence of scalars): El n\u00famero de contenedores (bins) o los l\u00edmites de los contenedores para la divisi\u00f3n.     figsize (tuple, opcional): El tama\u00f1o de la figura del gr\u00e1fico de barras. Por defecto es (8, 4).      Returns:     None     \"\"\"      # Configurar el estilo de Seaborn     sns.set(style='whitegrid')      # Agregar una nueva columna al DataFrame con los rangos de la columna espec\u00edfica     data[column + 'Range'] = pd.cut(data[column], bins=bins, right=False)      # Contar el n\u00famero de elementos en cada rango     temp_counts = data[column + 'Range'].value_counts().sort_index()      # Calcular los porcentajes en lugar de contar el n\u00famero de elementos     temp_percentages = (temp_counts / temp_counts.sum()) * 100  # Calcular los porcentajes relativos      # Crear el gr\u00e1fico de barras     plt.figure(figsize=figsize)     sns.barplot(x=temp_percentages.index, y=temp_percentages.values, color='#bce4b5', edgecolor='black')      # A\u00f1adir anotaciones de valores en las barras (porcentajes)     for i, value in enumerate(temp_percentages):         plt.text(i, value + 0.2, f'{value:.2f}%', ha='center', va='bottom', fontsize=9)      # Establecer t\u00edtulo y etiquetas de los ejes     plt.title(f'Distribuci\u00f3n por Rango de la columna {column}')     plt.xlabel('Rangos')     plt.ylabel('Frecuencia')     plt.xticks(rotation=0)  # Rotar las etiquetas del eje x para mayor legibilidad     plt.tight_layout()      # Mostrar el gr\u00e1fico     plt.show()      # eliminar columna extra     data.drop(column + 'Range',axis=1,inplace=True) In\u00a0[5]: Copied! <pre>def plot_range_distribution_vo(data, column, bins, vo, figsize=(8, 4)):\n    \"\"\"\n    Crea y muestra un gr\u00e1fico de barras que representa la distribuci\u00f3n de una columna dividida en rangos espec\u00edficos en un DataFrame,\n    agrupados por una variable objetivo y visualizando los porcentajes relativos en cada grupo.\n\n    Par\u00e1metros:\n    data (pandas DataFrame): El DataFrame que contiene los datos.\n    column (str): El nombre de la columna para la cual se crear\u00e1 la distribuci\u00f3n por rangos.\n    bins (int or sequence of scalars): El n\u00famero de contenedores (bins) o los l\u00edmites de los contenedores para la divisi\u00f3n.\n    vo (str): El nombre de la variable objetivo para agrupar los datos en el gr\u00e1fico de barras.\n    figsize (tuple, opcional): El tama\u00f1o de la figura del gr\u00e1fico de barras. Por defecto es (8, 4).\n\n    Returns:\n    None\n    \"\"\"\n\n    # Configurar el estilo de Seaborn\n    sns.set(style='whitegrid')\n\n    # Agregar una nueva columna al DataFrame con los rangos de la columna espec\u00edfica\n    data[column + 'Range'] = pd.cut(data[column], bins=bins, right=False)\n\n    # Calcular el conteo de cada grupo y reestructurar los datos\n    counts = data.groupby([vo, column + 'Range']).size().reset_index(name='Count')\n\n    # Calcular los porcentajes por categor\u00eda\n    counts['Percentage'] = counts.groupby(vo)['Count'].transform(lambda x: (x / x.sum()))\n\n    # Gr\u00e1fico de barras con Seaborn\n    plt.figure(figsize=figsize)\n    ax = sns.barplot(data=counts, x=column + 'Range', y='Percentage', hue=vo, palette='Greens', edgecolor='black')\n\n    # Rotar los ejes x (45 grados) y a\u00f1adir los valores en cada barra (excluyendo 0%)\n    for p in ax.patches:\n        if p.get_height() != 0:  # Si el valor no es 0%\n            ax.annotate(f'{p.get_height():.2%}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center',\n                        va='center', xytext=(0, 10), textcoords='offset points', fontsize=8)\n\n    # Establecer t\u00edtulo y etiquetas de los ejes\n    plt.title(f'Distribuci\u00f3n por Rango de la columna {column}')\n    plt.xlabel('Rangos')\n    plt.ylabel('Frecuencia')\n    plt.xticks(rotation=0)  # Rotar etiquetas del eje x para mejor legibilidad\n    plt.tight_layout()\n\n    # Mostrar el gr\u00e1fico\n    plt.show()\n\n    # eliminar columna extra\n    data.drop(column + 'Range', axis=1, inplace=True)\n</pre> def plot_range_distribution_vo(data, column, bins, vo, figsize=(8, 4)):     \"\"\"     Crea y muestra un gr\u00e1fico de barras que representa la distribuci\u00f3n de una columna dividida en rangos espec\u00edficos en un DataFrame,     agrupados por una variable objetivo y visualizando los porcentajes relativos en cada grupo.      Par\u00e1metros:     data (pandas DataFrame): El DataFrame que contiene los datos.     column (str): El nombre de la columna para la cual se crear\u00e1 la distribuci\u00f3n por rangos.     bins (int or sequence of scalars): El n\u00famero de contenedores (bins) o los l\u00edmites de los contenedores para la divisi\u00f3n.     vo (str): El nombre de la variable objetivo para agrupar los datos en el gr\u00e1fico de barras.     figsize (tuple, opcional): El tama\u00f1o de la figura del gr\u00e1fico de barras. Por defecto es (8, 4).      Returns:     None     \"\"\"      # Configurar el estilo de Seaborn     sns.set(style='whitegrid')      # Agregar una nueva columna al DataFrame con los rangos de la columna espec\u00edfica     data[column + 'Range'] = pd.cut(data[column], bins=bins, right=False)      # Calcular el conteo de cada grupo y reestructurar los datos     counts = data.groupby([vo, column + 'Range']).size().reset_index(name='Count')      # Calcular los porcentajes por categor\u00eda     counts['Percentage'] = counts.groupby(vo)['Count'].transform(lambda x: (x / x.sum()))      # Gr\u00e1fico de barras con Seaborn     plt.figure(figsize=figsize)     ax = sns.barplot(data=counts, x=column + 'Range', y='Percentage', hue=vo, palette='Greens', edgecolor='black')      # Rotar los ejes x (45 grados) y a\u00f1adir los valores en cada barra (excluyendo 0%)     for p in ax.patches:         if p.get_height() != 0:  # Si el valor no es 0%             ax.annotate(f'{p.get_height():.2%}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center',                         va='center', xytext=(0, 10), textcoords='offset points', fontsize=8)      # Establecer t\u00edtulo y etiquetas de los ejes     plt.title(f'Distribuci\u00f3n por Rango de la columna {column}')     plt.xlabel('Rangos')     plt.ylabel('Frecuencia')     plt.xticks(rotation=0)  # Rotar etiquetas del eje x para mejor legibilidad     plt.tight_layout()      # Mostrar el gr\u00e1fico     plt.show()      # eliminar columna extra     data.drop(column + 'Range', axis=1, inplace=True) In\u00a0[6]: Copied! <pre>def plot_barplot(data, column, figsize=(8, 4)):\n    \"\"\"\n    Crea y muestra un gr\u00e1fico de barras que representa la distribuci\u00f3n de una columna en un DataFrame.\n\n    Par\u00e1metros:\n    data (pandas DataFrame): El DataFrame que contiene los datos.\n    column (str): El nombre de la columna para la cual se crear\u00e1 el gr\u00e1fico de barras.\n    figsize (tuple, opcional): El tama\u00f1o de la figura del gr\u00e1fico de barras. Por defecto es (8, 4).\n\n    Returns:\n    None\n    \"\"\"\n\n    # Configurar el estilo de Seaborn\n    sns.set(style='whitegrid')\n\n    # Calcular los porcentajes de cada categor\u00eda en la columna especificada\n    temp_percentages = (data[column].value_counts(normalize=True) * 100).sort_index()\n\n    # Crear el gr\u00e1fico de barras con porcentajes\n    plt.figure(figsize=figsize)\n    temp_percentages.plot(kind='bar', color='#bce4b5', edgecolor='black')\n\n    # A\u00f1adir anotaciones de valores en las barras (porcentajes)\n    for i, value in enumerate(temp_percentages):\n        plt.text(i, value + 1, f'{value:.2f}%', ha='center', va='bottom', fontsize=9)\n\n    # Establecer t\u00edtulo y etiquetas de los ejes\n    plt.title(f'Distribuci\u00f3n de la columna {column}')\n    plt.xlabel(column)\n    plt.ylabel('Frecuencia')\n    plt.xticks(rotation=0)  # Rotar las etiquetas del eje x para mayor legibilidad\n    plt.ylim(0, 100)  # Establecer el rango del eje y de 0 a 100 para porcentajes\n    plt.show()\n</pre> def plot_barplot(data, column, figsize=(8, 4)):     \"\"\"     Crea y muestra un gr\u00e1fico de barras que representa la distribuci\u00f3n de una columna en un DataFrame.      Par\u00e1metros:     data (pandas DataFrame): El DataFrame que contiene los datos.     column (str): El nombre de la columna para la cual se crear\u00e1 el gr\u00e1fico de barras.     figsize (tuple, opcional): El tama\u00f1o de la figura del gr\u00e1fico de barras. Por defecto es (8, 4).      Returns:     None     \"\"\"      # Configurar el estilo de Seaborn     sns.set(style='whitegrid')      # Calcular los porcentajes de cada categor\u00eda en la columna especificada     temp_percentages = (data[column].value_counts(normalize=True) * 100).sort_index()      # Crear el gr\u00e1fico de barras con porcentajes     plt.figure(figsize=figsize)     temp_percentages.plot(kind='bar', color='#bce4b5', edgecolor='black')      # A\u00f1adir anotaciones de valores en las barras (porcentajes)     for i, value in enumerate(temp_percentages):         plt.text(i, value + 1, f'{value:.2f}%', ha='center', va='bottom', fontsize=9)      # Establecer t\u00edtulo y etiquetas de los ejes     plt.title(f'Distribuci\u00f3n de la columna {column}')     plt.xlabel(column)     plt.ylabel('Frecuencia')     plt.xticks(rotation=0)  # Rotar las etiquetas del eje x para mayor legibilidad     plt.ylim(0, 100)  # Establecer el rango del eje y de 0 a 100 para porcentajes     plt.show() In\u00a0[7]: Copied! <pre>def plot_barplot_vo(data, column, vo, figsize=(8, 4)):\n    \"\"\"\n    Crea y muestra un gr\u00e1fico de barras que representa la distribuci\u00f3n de una columna dividida por una variable objetivo.\n\n    Par\u00e1metros:\n    data (pandas DataFrame): El DataFrame que contiene los datos.\n    column (str): El nombre de la columna para la cual se crear\u00e1 el gr\u00e1fico de barras.\n    vo (str): El nombre de la variable objetivo para agrupar los datos en el gr\u00e1fico de barras.\n    figsize (tuple, opcional): El tama\u00f1o de la figura del gr\u00e1fico de barras. Por defecto es (8, 4).\n\n    Returns:\n    None\n    \"\"\"\n\n    # Configurar el estilo de Seaborn\n    sns.set(style='whitegrid')\n\n    # Calcular el conteo de cada grupo y reestructurar los datos\n    counts = data.groupby([vo, column]).size().reset_index(name='Count')\n\n    # Calcular los porcentajes por categor\u00eda\n    counts['Percentage'] = counts.groupby(vo)['Count'].transform(lambda x: (x / x.sum()))\n\n    # Gr\u00e1fico de barras con Seaborn\n    plt.figure(figsize=figsize)\n    ax = sns.barplot(data=counts, x=column, y='Percentage', hue=vo, palette='Greens', edgecolor='black')\n\n    # Rotar los ejes x (45 grados) y a\u00f1adir los valores en cada barra (excluyendo 0%)\n    for p in ax.patches:\n        if p.get_height() != 0:  # Si el valor no es 0%\n            ax.annotate(f'{p.get_height():.2%}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center',\n                        va='center', xytext=(0, 10), textcoords='offset points', fontsize=8)\n\n    # Establecer t\u00edtulo y etiquetas de los ejes\n    plt.title(f'Distribuci\u00f3n por Rango de la columna {column}')\n    plt.xlabel('Rangos')\n    plt.ylabel('Frecuencia')\n    plt.xticks(rotation=0)  # Rotar etiquetas del eje x para mejor legibilidad\n    plt.tight_layout()\n\n    # Mostrar el gr\u00e1fico\n    plt.show()\n</pre> def plot_barplot_vo(data, column, vo, figsize=(8, 4)):     \"\"\"     Crea y muestra un gr\u00e1fico de barras que representa la distribuci\u00f3n de una columna dividida por una variable objetivo.      Par\u00e1metros:     data (pandas DataFrame): El DataFrame que contiene los datos.     column (str): El nombre de la columna para la cual se crear\u00e1 el gr\u00e1fico de barras.     vo (str): El nombre de la variable objetivo para agrupar los datos en el gr\u00e1fico de barras.     figsize (tuple, opcional): El tama\u00f1o de la figura del gr\u00e1fico de barras. Por defecto es (8, 4).      Returns:     None     \"\"\"      # Configurar el estilo de Seaborn     sns.set(style='whitegrid')      # Calcular el conteo de cada grupo y reestructurar los datos     counts = data.groupby([vo, column]).size().reset_index(name='Count')      # Calcular los porcentajes por categor\u00eda     counts['Percentage'] = counts.groupby(vo)['Count'].transform(lambda x: (x / x.sum()))      # Gr\u00e1fico de barras con Seaborn     plt.figure(figsize=figsize)     ax = sns.barplot(data=counts, x=column, y='Percentage', hue=vo, palette='Greens', edgecolor='black')      # Rotar los ejes x (45 grados) y a\u00f1adir los valores en cada barra (excluyendo 0%)     for p in ax.patches:         if p.get_height() != 0:  # Si el valor no es 0%             ax.annotate(f'{p.get_height():.2%}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center',                         va='center', xytext=(0, 10), textcoords='offset points', fontsize=8)      # Establecer t\u00edtulo y etiquetas de los ejes     plt.title(f'Distribuci\u00f3n por Rango de la columna {column}')     plt.xlabel('Rangos')     plt.ylabel('Frecuencia')     plt.xticks(rotation=0)  # Rotar etiquetas del eje x para mejor legibilidad     plt.tight_layout()      # Mostrar el gr\u00e1fico     plt.show() In\u00a0[8]: Copied! <pre>def calculate_percentage_vo_int(data, column, vo):\n    \"\"\"\n    Calcula los porcentajes relativos de cada grupo dividido por una variable objetivo (vo) en un DataFrame,\n    manteniendo la columna de inter\u00e9s como \u00edndice en la tabla pivote.\n\n    Par\u00e1metros:\n    data (pandas DataFrame): El DataFrame que contiene los datos.\n    column (str): El nombre de la columna para la cual se calcular\u00e1n los porcentajes.\n    vo (str): El nombre de la variable objetivo para agrupar los datos y calcular los porcentajes.\n\n    Returns:\n    pandas DataFrame: Una tabla pivotante con los porcentajes relativos.\n    \"\"\"\n\n    # Calcular el conteo de cada grupo y reestructurar los datos\n    counts = data.groupby([vo, column]).size().reset_index(name='Count')\n\n    # Calcular los porcentajes por categor\u00eda\n    counts['Percentage'] = counts.groupby(vo)['Count'].transform(lambda x: (x / x.sum()))\n\n    # Crear una tabla pivote con los porcentajes\n    pivot_counts = counts.pivot_table(values=['Count', 'Percentage'], index=column, columns=vo)\n\n    return pivot_counts\n</pre> def calculate_percentage_vo_int(data, column, vo):     \"\"\"     Calcula los porcentajes relativos de cada grupo dividido por una variable objetivo (vo) en un DataFrame,     manteniendo la columna de inter\u00e9s como \u00edndice en la tabla pivote.      Par\u00e1metros:     data (pandas DataFrame): El DataFrame que contiene los datos.     column (str): El nombre de la columna para la cual se calcular\u00e1n los porcentajes.     vo (str): El nombre de la variable objetivo para agrupar los datos y calcular los porcentajes.      Returns:     pandas DataFrame: Una tabla pivotante con los porcentajes relativos.     \"\"\"      # Calcular el conteo de cada grupo y reestructurar los datos     counts = data.groupby([vo, column]).size().reset_index(name='Count')      # Calcular los porcentajes por categor\u00eda     counts['Percentage'] = counts.groupby(vo)['Count'].transform(lambda x: (x / x.sum()))      # Crear una tabla pivote con los porcentajes     pivot_counts = counts.pivot_table(values=['Count', 'Percentage'], index=column, columns=vo)      return pivot_counts In\u00a0[9]: Copied! <pre>def calculate_percentage_vo(data, column, bins, vo):\n    \"\"\"\n    Calcula los porcentajes relativos de cada grupo dividido por una variable objetivo (vo) en un DataFrame,\n    dentro de rangos espec\u00edficos definidos por column y bins.\n\n    Par\u00e1metros:\n    data (pandas DataFrame): El DataFrame que contiene los datos.\n    column (str): El nombre de la columna para la cual se calcular\u00e1n los porcentajes.\n    bins (int or sequence of scalars): El n\u00famero de contenedores (bins) o los l\u00edmites de los contenedores para la divisi\u00f3n.\n    vo (str): El nombre de la variable objetivo para agrupar los datos y calcular los porcentajes.\n\n    Returns:\n    pandas DataFrame: Una tabla pivotante con los porcentajes relativos.\n    \"\"\"\n\n    # Agregar una nueva columna al DataFrame con los rangos de la columna espec\u00edfica\n    data[column + 'Range'] = pd.cut(data[column], bins=bins, right=False)\n\n    # Calcular el conteo de cada grupo y reestructurar los datos\n    counts = data.groupby([vo, column + 'Range']).size().reset_index(name='Count')\n\n    # Calcular los porcentajes por categor\u00eda\n    counts['Percentage'] = counts.groupby(vo)['Count'].transform(lambda x: (x / x.sum()))\n\n    # Crear una tabla pivote con los porcentajes\n    pivot_counts = counts.pivot_table(values=['Count', 'Percentage'], index=column + 'Range', columns=vo)\n\n    # eliminar columna extra\n    data.drop(column + 'Range', axis=1, inplace=True)\n\n    return pivot_counts\n</pre> def calculate_percentage_vo(data, column, bins, vo):     \"\"\"     Calcula los porcentajes relativos de cada grupo dividido por una variable objetivo (vo) en un DataFrame,     dentro de rangos espec\u00edficos definidos por column y bins.      Par\u00e1metros:     data (pandas DataFrame): El DataFrame que contiene los datos.     column (str): El nombre de la columna para la cual se calcular\u00e1n los porcentajes.     bins (int or sequence of scalars): El n\u00famero de contenedores (bins) o los l\u00edmites de los contenedores para la divisi\u00f3n.     vo (str): El nombre de la variable objetivo para agrupar los datos y calcular los porcentajes.      Returns:     pandas DataFrame: Una tabla pivotante con los porcentajes relativos.     \"\"\"      # Agregar una nueva columna al DataFrame con los rangos de la columna espec\u00edfica     data[column + 'Range'] = pd.cut(data[column], bins=bins, right=False)      # Calcular el conteo de cada grupo y reestructurar los datos     counts = data.groupby([vo, column + 'Range']).size().reset_index(name='Count')      # Calcular los porcentajes por categor\u00eda     counts['Percentage'] = counts.groupby(vo)['Count'].transform(lambda x: (x / x.sum()))      # Crear una tabla pivote con los porcentajes     pivot_counts = counts.pivot_table(values=['Count', 'Percentage'], index=column + 'Range', columns=vo)      # eliminar columna extra     data.drop(column + 'Range', axis=1, inplace=True)      return pivot_counts In\u00a0[10]: Copied! <pre>logger.info(\"Leer Datos\")\n\n# paths\npath_raw = \"../../data/raw/\"\npath_procesed = \"../../data/procesed/\"\npath_final = \"../../data/final/\"\n</pre> logger.info(\"Leer Datos\")  # paths path_raw = \"../../data/raw/\" path_procesed = \"../../data/procesed/\" path_final = \"../../data/final/\" <pre>2024-04-02 22:32:22.585 | INFO     | __main__:&lt;module&gt;:1 - Leer Datos\n</pre> In\u00a0[11]: Copied! <pre># leer datos\ntrain = pd.read_csv(path_raw + \"train.csv\")\ntest = pd.read_csv(path_raw + \"test.csv\")\n</pre> # leer datos train = pd.read_csv(path_raw + \"train.csv\") test = pd.read_csv(path_raw + \"test.csv\") In\u00a0[12]: Copied! <pre># Informaci\u00f3n sobre los tipos de datos y valores no nulos en cada columna\nprint(\"TRAIN:\")\ntrain.info()\n</pre> # Informaci\u00f3n sobre los tipos de datos y valores no nulos en cada columna print(\"TRAIN:\") train.info() <pre>TRAIN:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB\n</pre> <p>Nota: Por ahora nos centraremos en el <code>train</code>, pero se debe hacer lo mismo para el <code>test</code></p> In\u00a0[13]: Copied! <pre># Verificar duplicados en el conjunto de datos\nduplicates = train.duplicated()\n\n# Contar el n\u00famero de filas duplicadas\nnum_duplicates = duplicates.sum()\nprint(\"N\u00famero de filas duplicadas:\", num_duplicates)\n</pre> # Verificar duplicados en el conjunto de datos duplicates = train.duplicated()  # Contar el n\u00famero de filas duplicadas num_duplicates = duplicates.sum() print(\"N\u00famero de filas duplicadas:\", num_duplicates) <pre>N\u00famero de filas duplicadas: 0\n</pre> <p>Para llevar a cabo un An\u00e1lisis Exploratorio de Datos (EDA) con gr\u00e1ficos, ya sea univariado, bivariado, u otros, es crucial considerar el tipo de datos con el que estemos trabajando. Adem\u00e1s, es posible realizar un an\u00e1lisis global de las columnas, como un an\u00e1lisis exhaustivo por columnas, dependiendo de la velocidad deseada para nuestro EDA. Se procurar\u00e1 presentar todos los casos de forma detallada y clara, buscando la m\u00e1xima comprensi\u00f3n posible.</p> In\u00a0[14]: Copied! <pre># Obtener nombres de columnas seg\u00fan tipos de datos\nvariable_objetivo = 'Survived'\n\ncolumnas_flotantes = [x for x in list(train.select_dtypes(include=['float64']).columns) if x!=variable_objetivo]\ncolumnas_enteras = [x for x in list(train.select_dtypes(include=['int32', 'int64']).columns) if x!=variable_objetivo] \ncolumnas_objetos =  [x for x in list(train.select_dtypes(include=['object']).columns) if x!=variable_objetivo] \n\n# Mostrar nombres de columnas por tipo de datos\nprint(f\"Variable Objetivo: {variable_objetivo}\")\nprint()\nprint(\"Total Columnas flotantes:\", len(columnas_flotantes))\nprint(\"Columnas flotantes:\", columnas_flotantes)\nprint()\nprint(\"Total Columnas enteras:\", len(columnas_enteras))\nprint(\"Columnas enteras:\", columnas_enteras)\nprint()\nprint(\"Total Columnas objetos:\", len(columnas_objetos))\nprint(\"Columnas objetos:\", columnas_objetos)\n</pre> # Obtener nombres de columnas seg\u00fan tipos de datos variable_objetivo = 'Survived'  columnas_flotantes = [x for x in list(train.select_dtypes(include=['float64']).columns) if x!=variable_objetivo] columnas_enteras = [x for x in list(train.select_dtypes(include=['int32', 'int64']).columns) if x!=variable_objetivo]  columnas_objetos =  [x for x in list(train.select_dtypes(include=['object']).columns) if x!=variable_objetivo]   # Mostrar nombres de columnas por tipo de datos print(f\"Variable Objetivo: {variable_objetivo}\") print() print(\"Total Columnas flotantes:\", len(columnas_flotantes)) print(\"Columnas flotantes:\", columnas_flotantes) print() print(\"Total Columnas enteras:\", len(columnas_enteras)) print(\"Columnas enteras:\", columnas_enteras) print() print(\"Total Columnas objetos:\", len(columnas_objetos)) print(\"Columnas objetos:\", columnas_objetos) <pre>Variable Objetivo: Survived\n\nTotal Columnas flotantes: 2\nColumnas flotantes: ['Age', 'Fare']\n\nTotal Columnas enteras: 4\nColumnas enteras: ['PassengerId', 'Pclass', 'SibSp', 'Parch']\n\nTotal Columnas objetos: 5\nColumnas objetos: ['Name', 'Sex', 'Ticket', 'Cabin', 'Embarked']\n</pre> <p>En este caso, la columna 'Age' deber\u00eda ser entera, pero tiene datos Nulos (o <code>NaN</code>), por lo cual se convierte autom\u00e1ticamente en una columna tipo <code>float</code>. De momento tratemosla como una columna tipo <code>float</code>.</p> In\u00a0[15]: Copied! <pre>logger.info(\"EDA\")\n</pre> logger.info(\"EDA\") <pre>2024-04-02 22:32:22.662 | INFO     | __main__:&lt;module&gt;:1 - EDA\n</pre> <p>Se disponen de columnas que sirven para identificar a cada individuo, com\u00fanmente identificadas por el t\u00e9rmino 'ID' en su nombre. Es esencial que estos identificadores no tengan el valor cero.</p> <p>Es importante notar que estos identificadores no deben estar duplicados, a menos que haya m\u00e1s de un registro debido al an\u00e1lisis en relaci\u00f3n con otras columnas (por ejemplo, el per\u00edodo). En estos casos espec\u00edficos, la duplicaci\u00f3n puede ser relevante y est\u00e1 asociada con ciertos contextos de an\u00e1lisis que involucran otras variables o per\u00edodos temporales.</p> In\u00a0[16]: Copied! <pre>logger.info('PassengerId')\ntotal_nulos = train['PassengerId'].isnull().sum()\nprint(f\"total de valores nulos: {total_nulos} \")\n</pre> logger.info('PassengerId') total_nulos = train['PassengerId'].isnull().sum() print(f\"total de valores nulos: {total_nulos} \") <pre>2024-04-02 22:32:22.677 | INFO     | __main__:&lt;module&gt;:1 - PassengerId\n</pre> <pre>total de valores nulos: 0 \n</pre> In\u00a0[17]: Copied! <pre># Dejar en el index\ntrain = train.set_index('PassengerId')\ntrain.head()\n</pre> # Dejar en el index train = train.set_index('PassengerId') train.head() Out[17]: Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked PassengerId 1 0 3 Braund, Mr. Owen Harris male 22.000 1 0 A/5 21171 7.250 NaN S 2 1 1 Cumings, Mrs. John Bradley (Florence Briggs Th... female 38.000 1 0 PC 17599 71.283 C85 C 3 1 3 Heikkinen, Miss. Laina female 26.000 0 0 STON/O2. 3101282 7.925 NaN S 4 1 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.000 1 0 113803 53.100 C123 S 5 0 3 Allen, Mr. William Henry male 35.000 0 0 373450 8.050 NaN S <p>Ahora, procederemos a trabajar el resto de las columnas:</p> In\u00a0[18]: Copied! <pre>logger.info(f\"flotantes:{columnas_flotantes}\")\nlogger.info('Age')\n</pre> logger.info(f\"flotantes:{columnas_flotantes}\") logger.info('Age') <pre>2024-04-02 22:32:22.708 | INFO     | __main__:&lt;module&gt;:1 - flotantes:['Age', 'Fare']\n2024-04-02 22:32:22.709 | INFO     | __main__:&lt;module&gt;:2 - Age\n</pre> In\u00a0[19]: Copied! <pre># Primer Caso: Crear el histograma para la columna Objetivo\nlogger.info(\"Primer Caso: histograma\")\n\n# Llamada a la funci\u00f3n\ncol = 'Age'\nplot_histogram(train, col)\n</pre> # Primer Caso: Crear el histograma para la columna Objetivo logger.info(\"Primer Caso: histograma\")  # Llamada a la funci\u00f3n col = 'Age' plot_histogram(train, col) <pre>2024-04-02 22:32:22.723 | INFO     | __main__:&lt;module&gt;:2 - Primer Caso: histograma\n</pre> In\u00a0[20]: Copied! <pre># Llamada a la funci\u00f3n \ncolumna = 'Age'\nvariable_objetivo = 'Survived'\nplot_histogram_vo(train, columna, variable_objetivo)\n</pre> # Llamada a la funci\u00f3n  columna = 'Age' variable_objetivo = 'Survived' plot_histogram_vo(train, columna, variable_objetivo) In\u00a0[21]: Copied! <pre># Llamada a la funci\u00f3n\nbins = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90]\ncolumna = 'Age'\nvariable_objetivo = 'Survived'\n\nplot_range_distribution(train, columna, bins, figsize=(10, 5))\n</pre> # Llamada a la funci\u00f3n bins = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90] columna = 'Age' variable_objetivo = 'Survived'  plot_range_distribution(train, columna, bins, figsize=(10, 5)) In\u00a0[22]: Copied! <pre># Llamada a la funci\u00f3n\nbins = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90]\ncolumna = 'Age'\nvariable_objetivo = 'Survived'\n\nplot_range_distribution_vo(train, col, bins, variable_objetivo, figsize=(10, 5))\n</pre> # Llamada a la funci\u00f3n bins = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90] columna = 'Age' variable_objetivo = 'Survived'  plot_range_distribution_vo(train, col, bins, variable_objetivo, figsize=(10, 5)) In\u00a0[23]: Copied! <pre># Llamada a la funci\u00f3n\nbins = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90]\ncolumna = 'Age'\nvariable_objetivo = 'Survived'\n\ntabla = calculate_percentage_vo(train, columna, bins, variable_objetivo)\ntabla\n</pre> # Llamada a la funci\u00f3n bins = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90] columna = 'Age' variable_objetivo = 'Survived'  tabla = calculate_percentage_vo(train, columna, bins, variable_objetivo) tabla Out[23]: Count Percentage Survived 0 1 0 1 AgeRange [0, 10) 24.000 38.000 0.057 0.131 [10, 20) 61.000 41.000 0.144 0.141 [20, 30) 143.000 77.000 0.337 0.266 [30, 40) 94.000 73.000 0.222 0.252 [40, 50) 55.000 34.000 0.130 0.117 [50, 60) 28.000 20.000 0.066 0.069 [60, 70) 13.000 6.000 0.031 0.021 [70, 80) 6.000 0.000 0.014 0.000 [80, 90) 0.000 1.000 0.000 0.003 <p>Para analizar variables con valores <code>float</code>, el enfoque depende de la distribuci\u00f3n de los datos. En el caso de un an\u00e1lisis univariado, se suele emplear un Histograma como punto de partida (Primer Caso). Sin embargo, si el conjunto de datos es extenso o si hay una concentraci\u00f3n notoria de valores alrededor de cero (un escenario com\u00fan), es m\u00e1s efectivo transformar los datos en intervalos discretos (Segundo Caso).</p> <p>La definici\u00f3n de estos intervalos puede hacerse de manera autom\u00e1tica mediante una funci\u00f3n que genere rangos equidistantes. No obstante, en ocasiones, es preferible definir manualmente estos intervalos, ya que la automatizaci\u00f3n podr\u00eda generar numerosos <code>bins</code>, dificultando la interpretaci\u00f3n y el an\u00e1lisis de los datos. En tales casos, un enfoque manual permite ajustar los intervalos de forma m\u00e1s acorde con la naturaleza espec\u00edfica de los datos, facilitando su comprensi\u00f3n y an\u00e1lisis.</p> In\u00a0[24]: Copied! <pre>logger.info('Fare')\n\n# Primer Caso: Crear el histograma para la columna Objetivo\nlogger.info(\"Primer Caso: histograma\")\n\n# Llamada a la funci\u00f3n\ncol = 'Fare'\nplot_histogram(train, col)\n</pre> logger.info('Fare')  # Primer Caso: Crear el histograma para la columna Objetivo logger.info(\"Primer Caso: histograma\")  # Llamada a la funci\u00f3n col = 'Fare' plot_histogram(train, col) <pre>2024-04-02 22:32:23.628 | INFO     | __main__:&lt;module&gt;:1 - Fare\n2024-04-02 22:32:23.629 | INFO     | __main__:&lt;module&gt;:4 - Primer Caso: histograma\n</pre> In\u00a0[25]: Copied! <pre># Llamada a la funci\u00f3n \ncolumna = 'Fare'\nvariable_objetivo = 'Survived'\nplot_histogram_vo(train, columna, variable_objetivo)\n</pre> # Llamada a la funci\u00f3n  columna = 'Fare' variable_objetivo = 'Survived' plot_histogram_vo(train, columna, variable_objetivo) In\u00a0[26]: Copied! <pre># Llamada a la funci\u00f3n\nbins = [0, 10, 25, 50, 100, 1000]\ncolumna = 'Fare'\nvariable_objetivo = 'Survived'\n\nplot_range_distribution(train, col, bins, figsize=(10, 5))\n</pre> # Llamada a la funci\u00f3n bins = [0, 10, 25, 50, 100, 1000] columna = 'Fare' variable_objetivo = 'Survived'  plot_range_distribution(train, col, bins, figsize=(10, 5)) In\u00a0[27]: Copied! <pre># Llamada a la funci\u00f3n\nbins = [0, 10, 25, 50, 100, 1000]\ncolumna = 'Fare'\nvariable_objetivo = 'Survived'\n\nplot_range_distribution_vo(train, col, bins, variable_objetivo, figsize=(10, 5))\n</pre> # Llamada a la funci\u00f3n bins = [0, 10, 25, 50, 100, 1000] columna = 'Fare' variable_objetivo = 'Survived'  plot_range_distribution_vo(train, col, bins, variable_objetivo, figsize=(10, 5)) In\u00a0[28]: Copied! <pre># Llamada a la funci\u00f3n\nbins = [0, 10, 25, 50, 100, 1000]\ncolumna = 'Fare'\nvariable_objetivo = 'Survived'\n\ntabla = calculate_percentage_vo(train, columna, bins, variable_objetivo)\ntabla\n</pre> # Llamada a la funci\u00f3n bins = [0, 10, 25, 50, 100, 1000] columna = 'Fare' variable_objetivo = 'Survived'  tabla = calculate_percentage_vo(train, columna, bins, variable_objetivo) tabla Out[28]: Count Percentage Survived 0 1 0 1 FareRange [0, 10) 269.000 67.000 0.490 0.196 [10, 25) 128.000 93.000 0.233 0.272 [25, 50) 100.000 73.000 0.182 0.213 [50, 100) 38.000 70.000 0.069 0.205 [100, 1000) 14.000 39.000 0.026 0.114 In\u00a0[29]: Copied! <pre>logger.info(f\"Enteras:{columnas_enteras}\")\nlogger.info(f\"Object:{columnas_objetos}\")\n</pre> logger.info(f\"Enteras:{columnas_enteras}\") logger.info(f\"Object:{columnas_objetos}\") <pre>2024-04-02 22:32:24.455 | INFO     | __main__:&lt;module&gt;:1 - Enteras:['PassengerId', 'Pclass', 'SibSp', 'Parch']\n2024-04-02 22:32:24.455 | INFO     | __main__:&lt;module&gt;:2 - Object:['Name', 'Sex', 'Ticket', 'Cabin', 'Embarked']\n</pre> <p>Para representar gr\u00e1ficamente variables de tipo <code>int</code> o <code>object</code>, se recomienda inicialmente utilizar el m\u00e9todo <code>value_counts</code> de Pandas para contar los valores \u00fanicos en esa columna. Sin embargo, existen diferentes consideraciones a tener en cuenta:</p> <ul> <li>En casos donde la cantidad de valores \u00fanicos es reducida, es apropiado emplear directamente <code>value_counts</code>, tanto para variables de tipo <code>int</code> como <code>object</code>.</li> <li>Para variables de tipo <code>int</code> con una extensa cantidad de categor\u00edas, es \u00fatil trabajar con intervalos de valores antes de la visualizaci\u00f3n gr\u00e1fica.</li> <li>En el caso de variables de tipo <code>object</code> con m\u00faltiples categor\u00edas, se puede considerar priorizar aquellas con mayor frecuencia y agrupar el resto bajo una etiqueta general como \"otros\". Sin embargo, si la mayor\u00eda de los valores son \u00fanicos, es posible que esa variable no aporte informaci\u00f3n relevante para ser representada en un gr\u00e1fico (por ejemplo, informaci\u00f3n como domicilio, n\u00fameros de tel\u00e9fono, correos electr\u00f3nicos, etc.).s.</li> </ul> In\u00a0[30]: Copied! <pre># Llamada a la funci\u00f3n \ncolumna = 'Pclass'\nlogger.info(columna)\nplot_barplot(train, columna)\n</pre> # Llamada a la funci\u00f3n  columna = 'Pclass' logger.info(columna) plot_barplot(train, columna) <pre>2024-04-02 22:32:24.471 | INFO     | __main__:&lt;module&gt;:3 - Pclass\n</pre> In\u00a0[31]: Copied! <pre>col = 'Pclass'\nvo = 'Survived'\nplot_barplot_vo(train, col, variable_objetivo, figsize=(10, 5))\n</pre> col = 'Pclass' vo = 'Survived' plot_barplot_vo(train, col, variable_objetivo, figsize=(10, 5)) In\u00a0[32]: Copied! <pre># Llamada a la funci\u00f3n \ncol = 'Pclass'\nvo = 'Survived'\ncalculate_percentage_vo_int(train,col,vo)\n</pre> # Llamada a la funci\u00f3n  col = 'Pclass' vo = 'Survived' calculate_percentage_vo_int(train,col,vo) Out[32]: Count Percentage Survived 0 1 0 1 Pclass 1 80.000 136.000 0.146 0.398 2 97.000 87.000 0.177 0.254 3 372.000 119.000 0.678 0.348 In\u00a0[33]: Copied! <pre># Llamada a la funci\u00f3n \ncolumna = 'SibSp'\nlogger.info(columna)\nplot_barplot(train, columna)\n</pre> # Llamada a la funci\u00f3n  columna = 'SibSp' logger.info(columna) plot_barplot(train, columna) <pre>2024-04-02 22:32:24.767 | INFO     | __main__:&lt;module&gt;:3 - SibSp\n</pre> In\u00a0[34]: Copied! <pre>col = 'SibSp'\nvo = 'Survived'\nplot_barplot_vo(train, col, variable_objetivo, figsize=(10, 5))\n</pre> col = 'SibSp' vo = 'Survived' plot_barplot_vo(train, col, variable_objetivo, figsize=(10, 5)) In\u00a0[35]: Copied! <pre># Llamada a la funci\u00f3n \ncol = 'SibSp'\nvo = 'Survived'\ncalculate_percentage_vo_int(train,col,vo)\n</pre> # Llamada a la funci\u00f3n  col = 'SibSp' vo = 'Survived' calculate_percentage_vo_int(train,col,vo) Out[35]: Count Percentage Survived 0 1 0 1 SibSp 0 398.000 210.000 0.725 0.614 1 97.000 112.000 0.177 0.327 2 15.000 13.000 0.027 0.038 3 12.000 4.000 0.022 0.012 4 15.000 3.000 0.027 0.009 5 5.000 NaN 0.009 NaN 8 7.000 NaN 0.013 NaN In\u00a0[36]: Copied! <pre># Llamada a la funci\u00f3n \ncolumna = 'Parch'\nlogger.info(columna)\nplot_barplot(train, columna)\n</pre> # Llamada a la funci\u00f3n  columna = 'Parch' logger.info(columna) plot_barplot(train, columna) <pre>2024-04-02 22:32:25.122 | INFO     | __main__:&lt;module&gt;:3 - Parch\n</pre> In\u00a0[37]: Copied! <pre>col = 'Parch'\nvo = 'Survived'\nplot_barplot_vo(train, col, variable_objetivo, figsize=(10, 5))\n</pre> col = 'Parch' vo = 'Survived' plot_barplot_vo(train, col, variable_objetivo, figsize=(10, 5)) In\u00a0[38]: Copied! <pre># Llamada a la funci\u00f3n \ncol = 'Parch'\nvo = 'Survived'\ncalculate_percentage_vo_int(train,col,vo).fillna(0)\n</pre> # Llamada a la funci\u00f3n  col = 'Parch' vo = 'Survived' calculate_percentage_vo_int(train,col,vo).fillna(0) Out[38]: Count Percentage Survived 0 1 0 1 Parch 0 445.000 233.000 0.811 0.681 1 53.000 65.000 0.097 0.190 2 40.000 40.000 0.073 0.117 3 2.000 3.000 0.004 0.009 4 4.000 0.000 0.007 0.000 5 4.000 1.000 0.007 0.003 6 1.000 0.000 0.002 0.000 In\u00a0[39]: Copied! <pre># Llamada a la funci\u00f3n \ncolumna = 'Sex'\nlogger.info(columna)\nplot_barplot(train, columna)\n</pre> # Llamada a la funci\u00f3n  columna = 'Sex' logger.info(columna) plot_barplot(train, columna) <pre>2024-04-02 22:32:25.557 | INFO     | __main__:&lt;module&gt;:3 - Sex\n</pre> In\u00a0[40]: Copied! <pre>col = 'Sex'\nvo = 'Survived'\nplot_barplot_vo(train, col, variable_objetivo, figsize=(10, 5))\n</pre> col = 'Sex' vo = 'Survived' plot_barplot_vo(train, col, variable_objetivo, figsize=(10, 5)) In\u00a0[41]: Copied! <pre># Llamada a la funci\u00f3n \ncol = 'Sex'\nvo = 'Survived'\ncalculate_percentage_vo_int(train,col,vo).fillna(0)\n</pre> # Llamada a la funci\u00f3n  col = 'Sex' vo = 'Survived' calculate_percentage_vo_int(train,col,vo).fillna(0) Out[41]: Count Percentage Survived 0 1 0 1 Sex female 81.000 233.000 0.148 0.681 male 468.000 109.000 0.852 0.319 In\u00a0[42]: Copied! <pre># Llamada a la funci\u00f3n \ncolumna = 'Embarked'\nlogger.info(columna)\nplot_barplot(train, columna)\n</pre> # Llamada a la funci\u00f3n  columna = 'Embarked' logger.info(columna) plot_barplot(train, columna) <pre>2024-04-02 22:32:25.849 | INFO     | __main__:&lt;module&gt;:3 - Embarked\n</pre> In\u00a0[43]: Copied! <pre>col = 'Embarked'\nvo = 'Survived'\nplot_barplot_vo(train, col, variable_objetivo, figsize=(10, 6))\n</pre> col = 'Embarked' vo = 'Survived' plot_barplot_vo(train, col, variable_objetivo, figsize=(10, 6)) In\u00a0[44]: Copied! <pre># Llamada a la funci\u00f3n \ncol = 'Embarked'\nvo = 'Survived'\ncalculate_percentage_vo_int(train,col,vo).fillna(0)\n</pre> # Llamada a la funci\u00f3n  col = 'Embarked' vo = 'Survived' calculate_percentage_vo_int(train,col,vo).fillna(0) Out[44]: Count Percentage Survived 0 1 0 1 Embarked C 75.000 93.000 0.137 0.274 Q 47.000 30.000 0.086 0.088 S 427.000 217.000 0.778 0.638 <p>Para realizar an\u00e1lisis exploratorios sobre las columnas 'Cabin', 'Name' y  'Ticket' del conjunto de datos del Titanic, puedes seguir varios enfoques dependiendo de la informaci\u00f3n que contengan y el objetivo espec\u00edfico de tu an\u00e1lisis.</p>"},{"location":"project/result_01/#lectura-de-datos","title":"Lectura de datos\u00b6","text":""},{"location":"project/result_01/#eda","title":"EDA\u00b6","text":""},{"location":"project/result_02/","title":"Result 02","text":"In\u00a0[1]: Copied! <pre># librerias\nfrom loguru import logger\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 500)\npd.set_option('display.float_format', lambda x: '%.3f' % x)\n</pre> # librerias from loguru import logger import pandas as pd  import matplotlib.pyplot as plt import seaborn as sns  import warnings warnings.filterwarnings(\"ignore\") pd.set_option('display.max_columns', 500) pd.set_option('display.max_rows', 500) pd.set_option('display.float_format', lambda x: '%.3f' % x) In\u00a0[2]: Copied! <pre>logger.info(\"Leer Datos\")\n\n# paths\npath_raw = \"../../data/raw/\"\npath_procesed = \"../../data/procesed/\"\npath_final = \"../../data/final/\"\n</pre> logger.info(\"Leer Datos\")  # paths path_raw = \"../../data/raw/\" path_procesed = \"../../data/procesed/\" path_final = \"../../data/final/\" <pre>2024-04-02 22:32:09.878 | INFO     | __main__:&lt;module&gt;:1 - Leer Datos\n</pre> In\u00a0[3]: Copied! <pre># leer datos\ntrain = pd.read_csv(path_raw + \"train.csv\")\ntest = pd.read_csv(path_raw + \"test.csv\")\n</pre> # leer datos train = pd.read_csv(path_raw + \"train.csv\") test = pd.read_csv(path_raw + \"test.csv\") In\u00a0[4]: Copied! <pre># Obtener nombres de columnas seg\u00fan tipos de datos\nvariable_objetivo = 'Survived'\n\ncolumnas_flotantes = [x for x in list(train.select_dtypes(include=['float64']).columns) if x!=variable_objetivo]\ncolumnas_enteras = [x for x in list(train.select_dtypes(include=['int32', 'int64']).columns) if x!=variable_objetivo] \ncolumnas_objetos =  [x for x in list(train.select_dtypes(include=['object']).columns) if x!=variable_objetivo]\n</pre> # Obtener nombres de columnas seg\u00fan tipos de datos variable_objetivo = 'Survived'  columnas_flotantes = [x for x in list(train.select_dtypes(include=['float64']).columns) if x!=variable_objetivo] columnas_enteras = [x for x in list(train.select_dtypes(include=['int32', 'int64']).columns) if x!=variable_objetivo]  columnas_objetos =  [x for x in list(train.select_dtypes(include=['object']).columns) if x!=variable_objetivo]  In\u00a0[5]: Copied! <pre>logger.info(\"eliminar variables: 'Name' y 'Ticket'\")\n\ncols_delete = ['Name', 'Ticket']\n\ntrain = train.drop(cols_delete,axis=1)\ntest = test.drop(cols_delete,axis=1)\n</pre> logger.info(\"eliminar variables: 'Name' y 'Ticket'\")  cols_delete = ['Name', 'Ticket']  train = train.drop(cols_delete,axis=1) test = test.drop(cols_delete,axis=1) <pre>2024-04-02 22:32:09.925 | INFO     | __main__:&lt;module&gt;:1 - eliminar variables: 'Name' y 'Ticket'\n</pre> In\u00a0[6]: Copied! <pre>logger.info(\"Rellenar 'Age' con el promedio\")\nage_mean = round(train['Age'].mean())\n\ntrain['Age'] = train['Age'].fillna(age_mean)\ntest['Age']  = test['Age'].fillna(age_mean)\n</pre> logger.info(\"Rellenar 'Age' con el promedio\") age_mean = round(train['Age'].mean())  train['Age'] = train['Age'].fillna(age_mean) test['Age']  = test['Age'].fillna(age_mean) <pre>2024-04-02 22:32:09.941 | INFO     | __main__:&lt;module&gt;:1 - Rellenar 'Age' con el promedio\n</pre> In\u00a0[7]: Copied! <pre>logger.info(\"Modificar y rellenar valores nulos 'Cabin'\")\ntrain['Cabin'] = train['Cabin'].fillna('N').str[0]\ntest['Cabin'] = test['Cabin'].fillna('N').str[0]\n</pre> logger.info(\"Modificar y rellenar valores nulos 'Cabin'\") train['Cabin'] = train['Cabin'].fillna('N').str[0] test['Cabin'] = test['Cabin'].fillna('N').str[0] <pre>2024-04-02 22:32:09.956 | INFO     | __main__:&lt;module&gt;:1 - Modificar y rellenar valores nulos 'Cabin'\n</pre> In\u00a0[8]: Copied! <pre>logger.info(\"Cambiar tipo de datos: 'Pclass', 'SibSp'y 'Parch'\")\n\ncolumns_to_convert = ['Pclass', 'SibSp','Parch']\ntrain[columns_to_convert] = train[columns_to_convert].astype(str)\ntest[columns_to_convert] = test[columns_to_convert].astype(str)\n</pre> logger.info(\"Cambiar tipo de datos: 'Pclass', 'SibSp'y 'Parch'\")  columns_to_convert = ['Pclass', 'SibSp','Parch'] train[columns_to_convert] = train[columns_to_convert].astype(str) test[columns_to_convert] = test[columns_to_convert].astype(str) <pre>2024-04-02 22:32:09.972 | INFO     | __main__:&lt;module&gt;:1 - Cambiar tipo de datos: 'Pclass', 'SibSp'y 'Parch'\n</pre> In\u00a0[9]: Copied! <pre>logger.info(\"Guardar Resultados\")\n\ntrain.to_csv(path_procesed+'train.csv',sep=',',index=False)\ntest.to_csv(path_procesed+'test.csv',sep=',',index=False)\n</pre> logger.info(\"Guardar Resultados\")  train.to_csv(path_procesed+'train.csv',sep=',',index=False) test.to_csv(path_procesed+'test.csv',sep=',',index=False) <pre>2024-04-02 22:32:09.987 | INFO     | __main__:&lt;module&gt;:1 - Guardar Resultados\n</pre>"},{"location":"project/result_02/#lectura-de-datos","title":"Lectura de datos\u00b6","text":""},{"location":"project/result_02/#feature-engineering","title":"Feature Engineering\u00b6","text":"<p>Esta etapa ofrece numerosas oportunidades para un an\u00e1lisis m\u00e1s profundo, especialmente al comparar con otras columnas. Sin embargo, para nuestros prop\u00f3sitos pr\u00e1cticos, seguiremos este enfoque:</p> <ul> <li>Eliminaremos las columnas 'Name' y 'Ticket', ya que inicialmente no contribuyen significativamente al modelo.</li> <li>Para la variable 'Age', rellenaremos los valores faltantes con la media de edades.</li> <li>Abordaremos la columna 'Cabin' al reemplazar los valores faltantes con el dato m\u00e1s frecuente, optimizando as\u00ed la integridad devariabler</li> <li>Modificaremos el tipo de las variables 'Pclass', 'SibSp'y 'Parch'.iable.</li> </ul>"},{"location":"project/result_03/","title":"Result 03","text":"In\u00a0[1]: Copied! <pre># librerias\nfrom loguru import logger\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 500)\npd.set_option('display.float_format', lambda x: '%.3f' % x)\n</pre> # librerias from loguru import logger import pandas as pd  import matplotlib.pyplot as plt import seaborn as sns  import warnings warnings.filterwarnings(\"ignore\") pd.set_option('display.max_columns', 500) pd.set_option('display.max_rows', 500) pd.set_option('display.float_format', lambda x: '%.3f' % x) In\u00a0[2]: Copied! <pre>logger.info(\"Leer Datos\")\n\n# paths\npath_raw = \"../../data/raw/\"\npath_procesed = \"../../data/procesed/\"\npath_final = \"../../data/final/\"\n</pre> logger.info(\"Leer Datos\")  # paths path_raw = \"../../data/raw/\" path_procesed = \"../../data/procesed/\" path_final = \"../../data/final/\" <pre>2024-04-02 22:32:36.747 | INFO     | __main__:&lt;module&gt;:1 - Leer Datos\n</pre> In\u00a0[3]: Copied! <pre># leer datos\ntrain = pd.read_csv(path_procesed + \"train.csv\")\ntest = pd.read_csv(path_procesed + \"test.csv\")\n</pre> # leer datos train = pd.read_csv(path_procesed + \"train.csv\") test = pd.read_csv(path_procesed + \"test.csv\") In\u00a0[4]: Copied! <pre>columns_to_convert = ['Pclass', 'SibSp','Parch']\ntrain[columns_to_convert] = train[columns_to_convert].astype(str)\ntest[columns_to_convert] = test[columns_to_convert].astype(str)\n</pre> columns_to_convert = ['Pclass', 'SibSp','Parch'] train[columns_to_convert] = train[columns_to_convert].astype(str) test[columns_to_convert] = test[columns_to_convert].astype(str) In\u00a0[5]: Copied! <pre># Obtener nombres de columnas seg\u00fan tipos de datos\nvariable_objetivo = 'Survived'\n\ncolumnas_flotantes = [x for x in list(train.select_dtypes(include=['float64']).columns) if x!=variable_objetivo]\ncolumnas_enteras = [x for x in list(train.select_dtypes(include=['int32', 'int64']).columns) if x!=variable_objetivo] \ncolumnas_objetos =  [x for x in list(train.select_dtypes(include=['object']).columns) if x!=variable_objetivo]\n</pre> # Obtener nombres de columnas seg\u00fan tipos de datos variable_objetivo = 'Survived'  columnas_flotantes = [x for x in list(train.select_dtypes(include=['float64']).columns) if x!=variable_objetivo] columnas_enteras = [x for x in list(train.select_dtypes(include=['int32', 'int64']).columns) if x!=variable_objetivo]  columnas_objetos =  [x for x in list(train.select_dtypes(include=['object']).columns) if x!=variable_objetivo]  In\u00a0[6]: Copied! <pre>import joblib\nimport time\n\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom lightgbm import LGBMClassifier\n\nfrom sklearn.metrics import (\n    accuracy_score,\n    precision_score,\n    recall_score,\n    f1_score,\n    roc_curve,\n    roc_auc_score,\n)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\n</pre> import joblib import time  from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier from sklearn.svm import SVC from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighborsClassifier from sklearn.tree import DecisionTreeClassifier from sklearn.naive_bayes import GaussianNB from sklearn.ensemble import AdaBoostClassifier from lightgbm import LGBMClassifier  from sklearn.metrics import (     accuracy_score,     precision_score,     recall_score,     f1_score,     roc_curve,     roc_auc_score, )  from sklearn.model_selection import train_test_split from sklearn.compose import ColumnTransformer from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.pipeline import Pipeline from sklearn.impute import SimpleImputer In\u00a0[7]: Copied! <pre>def train_and_evaluate_model(model, X_train, y_train, X_test, y_test):\n    start_time = time.time()\n    model.fit(X_train, y_train)\n    execution_time = time.time() - start_time\n\n    y_pred = model.predict(X_test)\n    accuracy = round(accuracy_score(y_test, y_pred), 3)\n    precision = round(precision_score(y_test, y_pred), 3)\n    recall = round(recall_score(y_test, y_pred), 3)\n    f1 = round(f1_score(y_test, y_pred), 3)\n\n    y_prob = model.predict_proba(X_test)[:, 1]\n    fpr, tpr, _ = roc_curve(y_test, y_prob, pos_label=-1)\n    auc = round(roc_auc_score(y_test, y_prob), 3)\n\n    evaluation_metrics = {\n        \"Accuracy\": accuracy,\n        \"Precision\": precision,\n        \"Recall\": recall,\n        \"F1-Score\": f1,\n        \"AUC\": auc,\n        \"Time\": round(execution_time, 3),\n    }\n\n    return evaluation_metrics\n\n\n# Funci\u00f3n para entrenar y evaluar cada modelo\ndef train_and_evaluate_all_models(models_dict, X_train, y_train, X_test, y_test):\n    evaluation_results = {}\n    for model_name, model in models_dict.items():\n        evaluation_metrics = train_and_evaluate_model(\n            model, X_train, y_train, X_test, y_test\n        )\n        evaluation_results[model_name] = evaluation_metrics\n\n    # Convertir los resultados en un DataFrame\n    results_df = pd.DataFrame.from_dict(evaluation_results, orient=\"index\")\n    return results_df\n</pre> def train_and_evaluate_model(model, X_train, y_train, X_test, y_test):     start_time = time.time()     model.fit(X_train, y_train)     execution_time = time.time() - start_time      y_pred = model.predict(X_test)     accuracy = round(accuracy_score(y_test, y_pred), 3)     precision = round(precision_score(y_test, y_pred), 3)     recall = round(recall_score(y_test, y_pred), 3)     f1 = round(f1_score(y_test, y_pred), 3)      y_prob = model.predict_proba(X_test)[:, 1]     fpr, tpr, _ = roc_curve(y_test, y_prob, pos_label=-1)     auc = round(roc_auc_score(y_test, y_prob), 3)      evaluation_metrics = {         \"Accuracy\": accuracy,         \"Precision\": precision,         \"Recall\": recall,         \"F1-Score\": f1,         \"AUC\": auc,         \"Time\": round(execution_time, 3),     }      return evaluation_metrics   # Funci\u00f3n para entrenar y evaluar cada modelo def train_and_evaluate_all_models(models_dict, X_train, y_train, X_test, y_test):     evaluation_results = {}     for model_name, model in models_dict.items():         evaluation_metrics = train_and_evaluate_model(             model, X_train, y_train, X_test, y_test         )         evaluation_results[model_name] = evaluation_metrics      # Convertir los resultados en un DataFrame     results_df = pd.DataFrame.from_dict(evaluation_results, orient=\"index\")     return results_df  In\u00a0[8]: Copied! <pre>logger.info(\"Dividir el conjunto de datos en entrenamiento y prueba\")\n\n# Dividir los datos en entrenamiento y prueba\nvo = 'Survived'\nset_index='PassengerId'\n\nfeatures = [x for x in train.columns if x not in [vo,set_index]]\n\nX = train[features]\ny = train[vo]\n\n# Dividir los datos en entrenamiento y prueba\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)\n</pre> logger.info(\"Dividir el conjunto de datos en entrenamiento y prueba\")  # Dividir los datos en entrenamiento y prueba vo = 'Survived' set_index='PassengerId'  features = [x for x in train.columns if x not in [vo,set_index]]  X = train[features] y = train[vo]  # Dividir los datos en entrenamiento y prueba X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42) <pre>2024-04-02 22:32:37.121 | INFO     | __main__:&lt;module&gt;:1 - Dividir el conjunto de datos en entrenamiento y prueba\n</pre> In\u00a0[9]: Copied! <pre># Ejemplo de variables num\u00e9ricas y categ\u00f3ricas\nnumeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\ncategorical_features = X.select_dtypes(include=['object']).columns.tolist()\n\n# Crear los transformadores para las variables num\u00e9ricas y categ\u00f3ricas\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Crear el ColumnTransformer para aplicar las transformaciones en un pipeline\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ])\n\n# Aplicar el preprocesamiento a los datos de entrenamiento y prueba\nX_train_processed = preprocessor.fit_transform(X_train)\nX_test_processed = preprocessor.transform(X_test)\n\n# Obtener los nombres de las columnas despu\u00e9s del preprocesamiento\nnumeric_feature_names = preprocessor.transformers_[0][-1]\ncategorical_feature_names = preprocessor.transformers_[1][-1]\n\n# Obtener las categor\u00edas \u00fanicas de las variables categ\u00f3ricas\nunique_categories = preprocessor.named_transformers_['cat']['onehot'].categories_\n\n# Crear los nombres de las columnas despu\u00e9s del OneHotEncoding\nencoded_categorical_feature_names = []\nfor i, categories in enumerate(unique_categories):\n    for category in categories:\n        encoded_categorical_feature_names.append(f'{categorical_feature_names[i]}_{category}')\n        \n# Convertir la matriz dispersa a un DataFrame de Pandas\ntransformed_train_df = pd.DataFrame(X_train_processed.toarray(), columns=numeric_feature_names+ encoded_categorical_feature_names)\ntransformed_test_df = pd.DataFrame(X_test_processed.toarray(), columns=numeric_feature_names+ encoded_categorical_feature_names)\n</pre> # Ejemplo de variables num\u00e9ricas y categ\u00f3ricas numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist() categorical_features = X.select_dtypes(include=['object']).columns.tolist()  # Crear los transformadores para las variables num\u00e9ricas y categ\u00f3ricas numeric_transformer = Pipeline(steps=[     ('imputer', SimpleImputer(strategy='median')),     ('scaler', StandardScaler()) ]) categorical_transformer = Pipeline(steps=[     ('imputer', SimpleImputer(strategy='most_frequent')),     ('onehot', OneHotEncoder(handle_unknown='ignore')) ])  # Crear el ColumnTransformer para aplicar las transformaciones en un pipeline preprocessor = ColumnTransformer(     transformers=[         ('num', numeric_transformer, numeric_features),         ('cat', categorical_transformer, categorical_features)     ])  # Aplicar el preprocesamiento a los datos de entrenamiento y prueba X_train_processed = preprocessor.fit_transform(X_train) X_test_processed = preprocessor.transform(X_test)  # Obtener los nombres de las columnas despu\u00e9s del preprocesamiento numeric_feature_names = preprocessor.transformers_[0][-1] categorical_feature_names = preprocessor.transformers_[1][-1]  # Obtener las categor\u00edas \u00fanicas de las variables categ\u00f3ricas unique_categories = preprocessor.named_transformers_['cat']['onehot'].categories_  # Crear los nombres de las columnas despu\u00e9s del OneHotEncoding encoded_categorical_feature_names = [] for i, categories in enumerate(unique_categories):     for category in categories:         encoded_categorical_feature_names.append(f'{categorical_feature_names[i]}_{category}')          # Convertir la matriz dispersa a un DataFrame de Pandas transformed_train_df = pd.DataFrame(X_train_processed.toarray(), columns=numeric_feature_names+ encoded_categorical_feature_names) transformed_test_df = pd.DataFrame(X_test_processed.toarray(), columns=numeric_feature_names+ encoded_categorical_feature_names) In\u00a0[10]: Copied! <pre># Modelos con mejores hiperpar\u00e1metros\n\n# Inicializaci\u00f3n del clasificador RandomForest con hiperpar\u00e1metros optimizados\nrandom_forest = RandomForestClassifier(random_state=42,\n                                        n_estimators=100,\n                                        max_depth=None,\n                                        min_samples_split=2,\n                                        min_samples_leaf=1)\n\n# Inicializaci\u00f3n del clasificador LGBM con hiperpar\u00e1metros optimizados\nlgbm = LGBMClassifier(random_state=42,\n                      n_estimators=100,\n                      learning_rate=0.1,\n                      max_depth=-1)\n\n# Inicializaci\u00f3n del clasificador DecisionTree con hiperpar\u00e1metros optimizados\ndecision_tree = DecisionTreeClassifier(random_state=42,\n                                       max_depth=None,\n                                       min_samples_split=2,\n                                       min_samples_leaf=1)\n\n# Inicializaci\u00f3n del clasificador KNeighbors con hiperpar\u00e1metros optimizados\nknn = KNeighborsClassifier(n_neighbors=5,\n                           weights='uniform',\n                           p=2)\n\n# Inicializaci\u00f3n del clasificador LogisticRegression con hiperpar\u00e1metros optimizados\nlogistic_regression = LogisticRegression(random_state=42,\n                                         C=1.0,\n                                         penalty='l2')\n\n\n# Inicializaci\u00f3n del clasificador GaussianNB con hiperpar\u00e1metros optimizados\ngaussian_nb = GaussianNB(var_smoothing=1e-9)\n\n# Inicializaci\u00f3n del clasificador AdaBoost con hiperpar\u00e1metros optimizados\nada_boost = AdaBoostClassifier(random_state=42,\n                               n_estimators=50,\n                               learning_rate=0.1)\n\n# Crear un diccionario de modelos con sus par\u00e1metros para facilitar la iteraci\u00f3n\nmodels = {\n    'Random Forest': random_forest, \n    'LGBM': lgbm,\n    'Decision Tree': decision_tree,\n    'KNN': knn, \n    'Logistic Regression': logistic_regression,\n    'GaussianNB': gaussian_nb,\n    'AdaBoost': ada_boost\n}\n</pre> # Modelos con mejores hiperpar\u00e1metros  # Inicializaci\u00f3n del clasificador RandomForest con hiperpar\u00e1metros optimizados random_forest = RandomForestClassifier(random_state=42,                                         n_estimators=100,                                         max_depth=None,                                         min_samples_split=2,                                         min_samples_leaf=1)  # Inicializaci\u00f3n del clasificador LGBM con hiperpar\u00e1metros optimizados lgbm = LGBMClassifier(random_state=42,                       n_estimators=100,                       learning_rate=0.1,                       max_depth=-1)  # Inicializaci\u00f3n del clasificador DecisionTree con hiperpar\u00e1metros optimizados decision_tree = DecisionTreeClassifier(random_state=42,                                        max_depth=None,                                        min_samples_split=2,                                        min_samples_leaf=1)  # Inicializaci\u00f3n del clasificador KNeighbors con hiperpar\u00e1metros optimizados knn = KNeighborsClassifier(n_neighbors=5,                            weights='uniform',                            p=2)  # Inicializaci\u00f3n del clasificador LogisticRegression con hiperpar\u00e1metros optimizados logistic_regression = LogisticRegression(random_state=42,                                          C=1.0,                                          penalty='l2')   # Inicializaci\u00f3n del clasificador GaussianNB con hiperpar\u00e1metros optimizados gaussian_nb = GaussianNB(var_smoothing=1e-9)  # Inicializaci\u00f3n del clasificador AdaBoost con hiperpar\u00e1metros optimizados ada_boost = AdaBoostClassifier(random_state=42,                                n_estimators=50,                                learning_rate=0.1)  # Crear un diccionario de modelos con sus par\u00e1metros para facilitar la iteraci\u00f3n models = {     'Random Forest': random_forest,      'LGBM': lgbm,     'Decision Tree': decision_tree,     'KNN': knn,      'Logistic Regression': logistic_regression,     'GaussianNB': gaussian_nb,     'AdaBoost': ada_boost }  In\u00a0[11]: Copied! <pre># Llamada a la funci\u00f3n para entrenar y evaluar todos los modelos\nlogger.info(\"Entrenar y evaluar todos los modelos\")\nresults_df = train_and_evaluate_all_models(models, transformed_train_df, y_train, transformed_test_df, y_test)\n</pre> # Llamada a la funci\u00f3n para entrenar y evaluar todos los modelos logger.info(\"Entrenar y evaluar todos los modelos\") results_df = train_and_evaluate_all_models(models, transformed_train_df, y_train, transformed_test_df, y_test) <pre>2024-04-02 22:32:37.183 | INFO     | __main__:&lt;module&gt;:2 - Entrenar y evaluar todos los modelos\n</pre> <pre>[LightGBM] [Info] Number of positive: 268, number of negative: 444\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000146 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 208\n[LightGBM] [Info] Number of data points in the train set: 712, number of used features: 20\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.376404 -&gt; initscore=-0.504838\n[LightGBM] [Info] Start training from score -0.504838\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n</pre> In\u00a0[12]: Copied! <pre># Mostrar el DataFrame con los resultados\nlogger.info(\"Ordenar los resultados por la m\u00e9trica AUC\")\nresults_df.sort_values('AUC',ascending = False)\n</pre> # Mostrar el DataFrame con los resultados logger.info(\"Ordenar los resultados por la m\u00e9trica AUC\") results_df.sort_values('AUC',ascending = False) <pre>2024-04-02 22:32:37.603 | INFO     | __main__:&lt;module&gt;:2 - Ordenar los resultados por la m\u00e9trica AUC\n</pre> Out[12]: Accuracy Precision Recall F1-Score AUC Time Random Forest 0.810 0.794 0.730 0.761 0.886 0.103 Logistic Regression 0.810 0.786 0.743 0.764 0.875 0.014 KNN 0.821 0.809 0.743 0.775 0.872 0.001 LGBM 0.793 0.768 0.716 0.741 0.871 0.101 AdaBoost 0.788 0.757 0.716 0.736 0.866 0.056 Decision Tree 0.788 0.743 0.743 0.743 0.788 0.003 GaussianNB 0.419 0.415 0.986 0.584 0.778 0.001 In\u00a0[13]: Copied! <pre>logger.info(\"Seleccionar modelo\")\n\n#model = LGBMClassifier(random_state=42)\nmodel = RandomForestClassifier(random_state=42)\n\nmodel.fit(transformed_train_df, y_train)\n</pre> logger.info(\"Seleccionar modelo\")  #model = LGBMClassifier(random_state=42) model = RandomForestClassifier(random_state=42)  model.fit(transformed_train_df, y_train) <pre>2024-04-02 22:32:37.619 | INFO     | __main__:&lt;module&gt;:1 - Seleccionar modelo\n</pre> Out[13]: <pre>RandomForestClassifier(random_state=42)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u00a0\u00a0RandomForestClassifier?Documentation for RandomForestClassifieriFitted<pre>RandomForestClassifier(random_state=42)</pre> In\u00a0[14]: Copied! <pre># Obtener los hiperpar\u00e1metros\nhiperparametros =  model.get_params()\nprint(hiperparametros)\n</pre> # Obtener los hiperpar\u00e1metros hiperparametros =  model.get_params() print(hiperparametros) <pre>{'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}\n</pre> In\u00a0[15]: Copied! <pre># Obtener la importancia de las caracter\u00edsticas\nfeature_importance = model.feature_importances_\n\n# Obtener los nombres de las caracter\u00edsticas\nfeature_names = transformed_train_df.columns\n\n# Crear un DataFrame con las caracter\u00edsticas y su importancia\nfeature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n\n# Ordenar el DataFrame por importancia en orden descendente\nfeature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n</pre> # Obtener la importancia de las caracter\u00edsticas feature_importance = model.feature_importances_  # Obtener los nombres de las caracter\u00edsticas feature_names = transformed_train_df.columns  # Crear un DataFrame con las caracter\u00edsticas y su importancia feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})  # Ordenar el DataFrame por importancia en orden descendente feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False) In\u00a0[16]: Copied! <pre># Mostrar las caracter\u00edsticas m\u00e1s importantes\nfeature_importance_df\n</pre> # Mostrar las caracter\u00edsticas m\u00e1s importantes feature_importance_df Out[16]: Feature Importance 0 Age 0.220 1 Fare 0.220 5 Sex_female 0.135 6 Sex_male 0.134 4 Pclass_3 0.044 28 Cabin_N 0.032 3 Pclass_2 0.020 2 Pclass_1 0.020 8 SibSp_1 0.017 32 Embarked_S 0.017 14 Parch_0 0.016 30 Embarked_C 0.016 7 SibSp_0 0.014 15 Parch_1 0.011 16 Parch_2 0.011 25 Cabin_E 0.010 31 Embarked_Q 0.009 22 Cabin_B 0.008 23 Cabin_C 0.006 10 SibSp_3 0.006 24 Cabin_D 0.006 9 SibSp_2 0.005 11 SibSp_4 0.004 21 Cabin_A 0.003 26 Cabin_F 0.003 13 SibSp_8 0.002 27 Cabin_G 0.002 18 Parch_4 0.002 19 Parch_5 0.001 12 SibSp_5 0.001 17 Parch_3 0.001 29 Cabin_T 0.000 20 Parch_6 0.000 In\u00a0[17]: Copied! <pre># Gr\u00e1fico de barras para visualizar las caracter\u00edsticas m\u00e1s importantes\nplt.figure(figsize=(10, 6))\nplt.barh(feature_importance_df['Feature'][:10], feature_importance_df['Importance'][:10])\nplt.xlabel('Importance')\nplt.title('Top 10 Feature Importance')\nplt.gca().invert_yaxis()  # Invertir el eje y para que las caracter\u00edsticas m\u00e1s importantes est\u00e9n en la parte superior\nplt.show()\n</pre> # Gr\u00e1fico de barras para visualizar las caracter\u00edsticas m\u00e1s importantes plt.figure(figsize=(10, 6)) plt.barh(feature_importance_df['Feature'][:10], feature_importance_df['Importance'][:10]) plt.xlabel('Importance') plt.title('Top 10 Feature Importance') plt.gca().invert_yaxis()  # Invertir el eje y para que las caracter\u00edsticas m\u00e1s importantes est\u00e9n en la parte superior plt.show() In\u00a0[18]: Copied! <pre>logger.info(\"Realizar predicciones\")\n\ndef preprocess_applier(preprocessor, X_data):\n    # Aplicar el preprocesamiento a los datos\n    X_data_processed = preprocessor.transform(X_data)\n\n    # Obtener los nombres de las columnas despu\u00e9s del preprocesamiento\n    numeric_feature_names = preprocessor.transformers_[0][-1]\n    categorical_feature_names = preprocessor.transformers_[1][-1]\n\n    # Obtener las categor\u00edas \u00fanicas de las variables categ\u00f3ricas\n    unique_categories = preprocessor.named_transformers_[\"cat\"][\"onehot\"].categories_\n\n    # Crear los nombres de las columnas despu\u00e9s del OneHotEncoding\n    encoded_categorical_feature_names = []\n    for i, categories in enumerate(unique_categories):\n        for category in categories:\n            encoded_categorical_feature_names.append(\n                f\"{categorical_feature_names[i]}_{category}\"\n            )\n\n    # Convertir la matriz dispersa a un DataFrame de Pandas\n    transformed_df = pd.DataFrame(\n        X_data_processed.toarray(),\n        columns=numeric_feature_names + encoded_categorical_feature_names,\n    )\n\n    return transformed_df\n\n\n\nX_test_processed2 = preprocess_applier(preprocessor, test.drop('PassengerId',axis=1))\npredictions = model.predict(X_test_processed2)\ntest[\"Survived\"] = predictions\ntest.head()\n</pre> logger.info(\"Realizar predicciones\")  def preprocess_applier(preprocessor, X_data):     # Aplicar el preprocesamiento a los datos     X_data_processed = preprocessor.transform(X_data)      # Obtener los nombres de las columnas despu\u00e9s del preprocesamiento     numeric_feature_names = preprocessor.transformers_[0][-1]     categorical_feature_names = preprocessor.transformers_[1][-1]      # Obtener las categor\u00edas \u00fanicas de las variables categ\u00f3ricas     unique_categories = preprocessor.named_transformers_[\"cat\"][\"onehot\"].categories_      # Crear los nombres de las columnas despu\u00e9s del OneHotEncoding     encoded_categorical_feature_names = []     for i, categories in enumerate(unique_categories):         for category in categories:             encoded_categorical_feature_names.append(                 f\"{categorical_feature_names[i]}_{category}\"             )      # Convertir la matriz dispersa a un DataFrame de Pandas     transformed_df = pd.DataFrame(         X_data_processed.toarray(),         columns=numeric_feature_names + encoded_categorical_feature_names,     )      return transformed_df    X_test_processed2 = preprocess_applier(preprocessor, test.drop('PassengerId',axis=1)) predictions = model.predict(X_test_processed2) test[\"Survived\"] = predictions test.head() <pre>2024-04-02 22:32:37.902 | INFO     | __main__:&lt;module&gt;:1 - Realizar predicciones\n</pre> Out[18]: PassengerId Pclass Sex Age SibSp Parch Fare Cabin Embarked Survived 0 892 3 male 34.500 0 0 7.829 N Q 0 1 893 3 female 47.000 1 0 7.000 N S 0 2 894 2 male 62.000 0 0 9.688 N Q 0 3 895 3 male 27.000 0 0 8.662 N S 1 4 896 3 female 22.000 1 1 12.287 N S 0 In\u00a0[19]: Copied! <pre>logger.info(\"Guardar Resultados\")\n</pre> logger.info(\"Guardar Resultados\") <pre>2024-04-02 22:32:37.931 | INFO     | __main__:&lt;module&gt;:1 - Guardar Resultados\n</pre> In\u00a0[20]: Copied! <pre>logger.info(\"Guardar resultados de las predicciones\")\n\ntest.to_csv(path_final + \"predictions.csv\",index=False,sep=',')\n</pre> logger.info(\"Guardar resultados de las predicciones\")  test.to_csv(path_final + \"predictions.csv\",index=False,sep=',') <pre>2024-04-02 22:32:37.946 | INFO     | __main__:&lt;module&gt;:1 - Guardar resultados de las predicciones\n</pre> In\u00a0[21]: Copied! <pre>logger.info(\"Guardar resultados de los distintos modelos\")\n\nresults_df.to_csv(path_final + \"models_metrics.csv\",index=False,sep=',')\n</pre> logger.info(\"Guardar resultados de los distintos modelos\")  results_df.to_csv(path_final + \"models_metrics.csv\",index=False,sep=',') <pre>2024-04-02 22:32:37.961 | INFO     | __main__:&lt;module&gt;:1 - Guardar resultados de los distintos modelos\n</pre> In\u00a0[22]: Copied! <pre>logger.info(\"Guardar Modelo\" )\npath_models = \"../models/\"\njoblib.dump(model, path_models + 'modelo_titanic_rf.pkl')\n</pre> logger.info(\"Guardar Modelo\" ) path_models = \"../models/\" joblib.dump(model, path_models + 'modelo_titanic_rf.pkl') <pre>2024-04-02 22:32:37.976 | INFO     | __main__:&lt;module&gt;:1 - Guardar Modelo\n</pre> Out[22]: <pre>['../models/modelo_titanic_rf.pkl']</pre>"},{"location":"project/result_03/#lectura-de-datos","title":"Lectura de datos\u00b6","text":""},{"location":"project/result_03/#modelos","title":"Modelos\u00b6","text":""}]}